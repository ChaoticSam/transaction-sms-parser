{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1S3f1Yuk_aM9xtu5vXeGZPSpHfASqtYTw",
      "authorship_tag": "ABX9TyM1SvyxuiaqVfqbup7dtUnp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChaoticSam/transaction-sms-parser/blob/v0.1/sms_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’¬ SMS Transaction Parser - NLP Pipeline\n",
        "\n",
        "This project builds a robust NLP pipeline to parse raw SMS alerts and extract structured financial information.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Objective\n",
        "\n",
        "Classify SMS messages into categories and extract structured data fields:\n",
        "\n",
        "- **Classification**: Identify whether the SMS is about a:\n",
        "  - `debit` transaction\n",
        "  - `credit` transaction\n",
        "  \n",
        "- **Entity Extraction**:\n",
        "  - `amount`: Transaction amount (e.g., â‚¹5,000)\n",
        "  - `balance`: Available balance if present (e.g., Avl Bal: â‚¹10,000)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ› ï¸ Project Workflow\n",
        "\n",
        "### 1. Preprocessing\n",
        "- Lemmatization using `spaCy`\n",
        "- Removal of stopwords and punctuation\n",
        "- Lowercasing text\n",
        "\n",
        "### 2. Feature Engineering\n",
        "- `TF-IDF` vectorization (unigrams + bigrams)\n",
        "- Binary keyword features:\n",
        "  - `has_fail`, `has_due`, `has_otp`, `has_credit_score`, `has_payment`, `has_credit_card`\n",
        "\n",
        "### 3. Classification Model\n",
        "- **Algorithm**: `RandomForestClassifier`\n",
        "- **Input**: TF-IDF + binary features\n",
        "- **Output**: `debit`, `credit`, or `misc`\n",
        "- **Metrics**:\n",
        "  - Accuracy\n",
        "  - Precision, Recall, F1-score\n",
        "\n",
        "### 4. Named Entity Recognition (NER)\n",
        "- Trained a custom `spaCy` model to detect:\n",
        "  - `AMOUNT` â€” Transaction amount\n",
        "  - `BALANCE` â€” Available balance\n",
        "- Annotated ~150+ real and synthetic SMS messages\n",
        "\n",
        "### 5. Inference Pipeline\n",
        "- Load Acutal SMS Test dataset (Data.xlsx)\n",
        "- Predict class (`debit`, `credit`, `misc`)\n",
        "- Extract `amount` and `balance` using trained `spaCy` NER\n",
        "- Save output to `.csv` or display in notebook\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  How to Run\n",
        "\n",
        "1. Install dependencies:\n",
        "```bash\n",
        "pip install spacy pandas scikit-learn joblib\n",
        "python -m spacy download en_core_web_sm\n",
        "\n",
        "2. Train Classifier (if needed):\n",
        "```bash\n",
        "train_pipeline(\"data/training_data.csv\")\n",
        "\n",
        "3. Call prediction function:\n",
        "```bash\n",
        "predict_sms(\"data/Data.csv\")\n"
      ],
      "metadata": {
        "id": "RmyP3u4YSe_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import numpy as np\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "QEY96uo5TXqR"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GLOBAL PARAMETERS\n",
        "SPACY_MODEL = \"output/model-best\"\n",
        "TFIDF_PATH = \"/content/models/sms_tfidf.pkl\"\n",
        "MODEL_PATH = \"/content/models/sms_rf_model.pkl\"\n",
        "LABEL_MAP_PATH = \"/content/models/label_map.pkl\""
      ],
      "metadata": {
        "id": "kSalpGkPT9AV"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SMS PREPROCESSING\n",
        "def preprocess_text(text: str) -> str:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text or \"\")\n",
        "    tokens = [tok.lemma_ for tok in doc if not tok.is_stop and not tok.is_punct]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "YGZBWhlxT_w5"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTITY EXTRACT (AMOUNT AND BALANCE)\n",
        "def extract_fields(text: str) -> dict:\n",
        "    nlp = spacy.load(SPACY_MODEL)\n",
        "    doc = nlp(text)\n",
        "    amount = None\n",
        "    balance = None\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"AMOUNT\":\n",
        "            amount = ent.text.strip(\"â‚¹Rs. \").replace(\",\", \"\")\n",
        "        elif ent.label_ == \"BALANCE\":\n",
        "            balance = ent.text.strip(\"â‚¹Rs. \").replace(\",\", \"\")\n",
        "\n",
        "    return {\"amount\": amount, \"balance\": balance}"
      ],
      "metadata": {
        "id": "lhGnAUPRUEgA"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING PIPELINE FOR CLASSIFICATION\n",
        "\n",
        "def train_pipeline(data_path: str):\n",
        "    df = pd.read_excel(data_path) if data_path.lower().endswith(('xls', 'xlsx')) else pd.read_csv(data_path)\n",
        "    df['clean'] = df['SMS'].apply(preprocess_text)\n",
        "\n",
        "    # Adding features\n",
        "    df[\"has_fail\"] = df[\"SMS\"].str.contains(r\"\\b(?:fail|invalid|declined)\\b\", case=False).astype(int)\n",
        "    df[\"has_due\"] = df[\"SMS\"].str.contains(r\"\\b(?:due|overdue|late|charge|emi)\\b\", case=False).astype(int)\n",
        "    df[\"has_otp\"] = df[\"SMS\"].str.contains(r\"\\b(?:otp|verify|code|login)\\b\", case=False).astype(int)\n",
        "    df[\"has_credit_score\"] = df[\"SMS\"].str.contains(r\"\\b(?:credit score|cibil|score)\\b\", case=False).astype(int)\n",
        "    df['has_payment'] = df['SMS'].str.contains(r'\\b(?:payment|paid|successfully processed|processed|debited)\\b',case=False,regex=True).astype(int)\n",
        "    df['has_credit_card'] = df['SMS'].str.contains(r'\\b(?:credit card)\\b',case=False,regex=True).astype(int)\n",
        "\n",
        "    # Vectorizing text\n",
        "    tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=3000)\n",
        "    X_text = tfidf.fit_transform(df['clean'])\n",
        "    X_bin = df[[\"has_fail\",\"has_due\",\"has_otp\",\"has_credit_score\", \"has_payment\", \"has_credit_card\"]].values\n",
        "    X = hstack([X_text, csr_matrix(X_bin)])\n",
        "\n",
        "    # # Encoding labels\n",
        "    label_map = {\"debit\":0, \"credit\":1, \"misc\":2}\n",
        "    y = df[\"label\"].map(label_map)\n",
        "\n",
        "    # Training RandomForest on full data\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=20,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, stratify=y, random_state=42)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(classification_report(y_test, y_pred, target_names=label_map.keys()))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Persist\n",
        "    joblib.dump(tfidf, TFIDF_PATH)\n",
        "    joblib.dump(model, MODEL_PATH)\n",
        "    joblib.dump(label_map, LABEL_MAP_PATH)\n",
        "    print(f\"Saved TFIDF, model, and label map to disk.\")"
      ],
      "metadata": {
        "id": "w6b2LeWGUJ7r"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASSIFICATION LOGIC\n",
        "\n",
        "def predict_sms(data_path: str) -> pd.DataFrame:\n",
        "    df = pd.read_excel(data_path) if data_path.lower().endswith(('xls', 'xlsx')) else pd.read_csv(data_path)\n",
        "    df['clean'] = df['SMS'].apply(preprocess_text)\n",
        "\n",
        "    # Load artifacts\n",
        "    tfidf = joblib.load(TFIDF_PATH)\n",
        "    model = joblib.load(MODEL_PATH)\n",
        "    label_map = joblib.load(LABEL_MAP_PATH)\n",
        "    reverse_map = {v:k for k,v in label_map.items()}\n",
        "    df[\"has_fail\"] = df[\"SMS\"].str.contains(r\"\\b(?:fail|invalid|declined)\\b\", case=False).astype(int)\n",
        "    df[\"has_due\"] = df[\"SMS\"].str.contains(r\"\\b(?:due|overdue|late|charge|emi)\\b\", case=False).astype(int)\n",
        "    df[\"has_otp\"] = df[\"SMS\"].str.contains(r\"\\b(?:otp|verify|code|login)\\b\", case=False).astype(int)\n",
        "    df[\"has_credit_score\"] = df[\"SMS\"].str.contains(r\"\\b(?:credit score|cibil|score)\\b\", case=False).astype(int)\n",
        "    df['has_payment'] = df['SMS'].str.contains(r'\\b(?:payment|paid|successfully processed|processed|debited)\\b',case=False,regex=True).astype(int)\n",
        "    df['has_credit_card'] = df['SMS'].str.contains(r'\\b(?:credit card)\\b',case=False,regex=True).astype(int)\n",
        "\n",
        "    # Features\n",
        "    X_text = tfidf.transform(df['clean'])\n",
        "    X_bin  = df[[\"has_fail\",\"has_due\",\"has_otp\",\"has_credit_score\", \"has_payment\", \"has_credit_card\"]].values\n",
        "    X = hstack([X_text, X_bin])\n",
        "\n",
        "    # Classification\n",
        "    pred_ids = model.predict(X)\n",
        "    df[\"predicted_label\"] = [reverse_map[i] for i in pred_ids]\n",
        "\n",
        "    # Field extraction only if predicted_label is credit or debit\n",
        "    def conditional_extract(row):\n",
        "        if row['predicted_label'] in ['credit', 'debit']:\n",
        "            return extract_fields(row['SMS'])\n",
        "        else:\n",
        "            return pd.Series({'amount': None, 'balance': None})\n",
        "\n",
        "    fields = df.apply(conditional_extract, axis=1)\n",
        "    df = pd.concat([df, fields], axis=1)\n",
        "\n",
        "    return df[['SMS', 'predicted_label', 'amount', 'balance']]"
      ],
      "metadata": {
        "id": "d7aSBKz3UMTw"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Classification model\n",
        "train_pipeline('/content/data/training_data.csv')\n",
        "\n",
        "# Classify and Saving Final Output\n",
        "df = predict_sms('/content/data/Data.csv')\n",
        "df.to_csv(\"/content/response/final_output.csv\", index=False)\n",
        "print(\"Final output saved to response/final_output.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZwJFVsjWgYk",
        "outputId": "774b6668-b2cc-41ee-ba69-8e2a91e19cb3"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Final output saved to final_output.csv\n"
          ]
        }
      ]
    }
  ]
}